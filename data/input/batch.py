import requests
import json
import time
import threading
from threading import Lock
from queue import Queue
import argparse
from typing import List, Dict, Any
import os

class URLScraper:
    def __init__(self, api_url: str = "http://localhost:5000", api_key: str = "demo-api-key-123"):
        self.api_url = api_url.rstrip('/')
        self.api_key = api_key
        self.session = requests.Session()
        self.session.headers.update({"X-API-Key": self.api_key})
        
        # å­˜å‚¨ç»“æœ
        self.results = []
        self.results_lock = Lock()
        
        # ä»»åŠ¡é˜Ÿåˆ—
        self.task_queue = Queue()
        self.completed_tasks = set()
        
    def check_health(self) -> bool:
        """æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€"""
        try:
            response = self.session.get(f"{self.api_url}/health", timeout=10)
            return response.status_code == 200
        except requests.exceptions.RequestException:
            return False
    
    def submit_scraping_task(self, urls: List[str]) -> str:
        """æäº¤çˆ¬å–ä»»åŠ¡"""
        data = {
            "urls": urls,
            "scraper_type": "requests"
        }
        
        try:
            response = self.session.post(
                f"{self.api_url}/api/v1/scrape",
                json=data,
                timeout=30
            )
            response.raise_for_status()
            result = response.json()
            return result.get("task_id")
        except requests.exceptions.RequestException as e:
            print(f"æäº¤ä»»åŠ¡å¤±è´¥: {e}")
            return None
    
    def get_task_status(self, task_id: str) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        try:
            response = self.session.get(
                f"{self.api_url}/api/v1/tasks/{task_id}",
                timeout=10
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥ {task_id}: {e}")
            return {"status": "error", "error": str(e)}
    
    def get_task_results(self, task_id: str) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡ç»“æœ"""
        try:
            response = self.session.get(
                f"{self.api_url}/api/v1/tasks/{task_id}/results",
                timeout=10
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"è·å–ä»»åŠ¡ç»“æœå¤±è´¥ {task_id}: {e}")
            return {"error": str(e)}
    
    def process_single_url(self, url: str, task_id: str):
        """å¤„ç†å•ä¸ªURLçš„ä»»åŠ¡çŠ¶æ€è½®è¯¢"""
        max_retries = 60  # æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆ5åˆ†é’Ÿï¼‰
        retry_count = 0
        
        while retry_count < max_retries:
            # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
            status_info = self.get_task_status(task_id)
            status = status_info.get("status", "unknown")
            
            if status in ["completed", "failed"]:
                # è·å–æœ€ç»ˆç»“æœ
                results = self.get_task_results(task_id)
                
                with self.results_lock:
                    self.results.append({
                        "url": url,
                        "task_id": task_id,
                        "status": status,
                        "results": results,
                        "timestamp": time.time()
                    })
                
                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                self.completed_tasks.add(url)
                
                # è¾“å‡ºç»“æœ
                if status == "completed":
                    print(f"âœ… æˆåŠŸ: {url} (ä»»åŠ¡ID: {task_id})")
                else:
                    print(f"âŒ å¤±è´¥: {url} (ä»»åŠ¡ID: {task_id}) - çŠ¶æ€: {status}")
                
                break
            elif status == "in_progress":
                print(f"â³ å¤„ç†ä¸­: {url} (ä»»åŠ¡ID: {task_id})")
            else:
                print(f"ğŸ”„ ç­‰å¾…ä¸­: {url} (ä»»åŠ¡ID: {task_id}) - çŠ¶æ€: {status}")
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
            time.sleep(5)
            retry_count += 1
        
        if retry_count >= max_retries:
            print(f"â° è¶…æ—¶: {url} (ä»»åŠ¡ID: {task_id})")
            with self.results_lock:
                self.results.append({
                    "url": url,
                    "task_id": task_id,
                    "status": "timeout",
                    "results": {"error": "è½®è¯¢è¶…æ—¶"},
                    "timestamp": time.time()
                })
            self.completed_tasks.add(url)
    
    def worker(self):
        """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
        while True:
            url, task_id = self.task_queue.get()
            if url is None:  # é€€å‡ºä¿¡å·
                self.task_queue.task_done()
                break
            
            self.process_single_url(url, task_id)
            self.task_queue.task_done()
    
    def process_urls_from_file(self, filename: str, num_threads: int = 3):
        """ä»æ–‡ä»¶è¯»å–URLå¹¶å¤„ç†"""
        # è¯»å–URLæ–‡ä»¶
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"é”™è¯¯: æ–‡ä»¶ '{filename}' ä¸å­˜åœ¨")
            return
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶é”™è¯¯: {e}")
            return
        
        if not urls:
            print("é”™è¯¯: æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆçš„URL")
            return
        
        print(f"ğŸ“– ä»æ–‡ä»¶è¯»å–åˆ° {len(urls)} ä¸ªURL")
        
        # æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€
        print("ğŸ” æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€...")
        if not self.check_health():
            print("âŒ æœåŠ¡å™¨ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨")
            return
        print("âœ… æœåŠ¡å™¨è¿æ¥æ­£å¸¸")
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        print(f"ğŸš€ å¯åŠ¨ {num_threads} ä¸ªå·¥ä½œçº¿ç¨‹...")
        threads = []
        for i in range(num_threads):
            thread = threading.Thread(target=self.worker)
            thread.daemon = True
            thread.start()
            threads.append(thread)
        
        # åˆ†æ‰¹æäº¤URLï¼ˆé¿å…ä¸€æ¬¡æ€§æäº¤å¤ªå¤šï¼‰
        batch_size = 10
        total_submitted = 0
        
        for i in range(0, len(urls), batch_size):
            batch_urls = urls[i:i + batch_size]
            print(f"ğŸ“¤ æäº¤æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch_urls)} ä¸ªURL")
            
            # æäº¤ä»»åŠ¡
            task_id = self.submit_scraping_task(batch_urls)
            if task_id:
                # å°†æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªURLå’Œä»»åŠ¡IDåŠ å…¥é˜Ÿåˆ—
                for url in batch_urls:
                    self.task_queue.put((url, task_id))
                    total_submitted += 1
            else:
                print(f"âŒ æ‰¹æ¬¡ {i//batch_size + 1} æäº¤å¤±è´¥")
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…æœåŠ¡å™¨å‹åŠ›è¿‡å¤§
            time.sleep(2)
        
        print(f"âœ… å·²æäº¤ {total_submitted} ä¸ªURLåˆ°å¤„ç†é˜Ÿåˆ—")
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        self.task_queue.join()
        
        # å‘é€é€€å‡ºä¿¡å·ç»™å·¥ä½œçº¿ç¨‹
        for _ in range(num_threads):
            self.task_queue.put((None, None))
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ç»“æŸ
        for thread in threads:
            thread.join()
        
        print("ğŸ‰ æ‰€æœ‰URLå¤„ç†å®Œæˆ!")
        
        # ä¿å­˜ç»“æœ
        self.save_results()
    
    def save_results(self):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_files = []
        
        # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆJSONæ ¼å¼ï¼‰
        detailed_filename = f"scraping_results_{timestamp}.json"
        with open(detailed_filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        output_files.append(detailed_filename)
        
        # ä¿å­˜æ‘˜è¦ç»“æœï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
        summary_filename = f"scraping_summary_{timestamp}.txt"
        with open(summary_filename, 'w', encoding='utf-8') as f:
            f.write(f"URLçˆ¬å–ç»“æœæ‘˜è¦ - {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 50 + "\n\n")
            
            success_count = sum(1 for r in self.results if r['status'] == 'completed')
            failed_count = sum(1 for r in self.results if r['status'] in ['failed', 'timeout'])
            
            f.write(f"æ€»è®¡URLæ•°é‡: {len(self.results)}\n")
            f.write(f"æˆåŠŸæ•°é‡: {success_count}\n")
            f.write(f"å¤±è´¥æ•°é‡: {failed_count}\n")
            f.write(f"æˆåŠŸç‡: {success_count/len(self.results)*100:.1f}%\n\n")
            
            f.write("æˆåŠŸURLåˆ—è¡¨:\n")
            for result in self.results:
                if result['status'] == 'completed':
                    f.write(f"âœ… {result['url']}\n")
            
            f.write("\nå¤±è´¥URLåˆ—è¡¨:\n")
            for result in self.results:
                if result['status'] in ['failed', 'timeout']:
                    f.write(f"âŒ {result['url']} ({result['status']})\n")
        
        output_files.append(summary_filename)
        
        # ä¿å­˜æˆåŠŸURLåˆ—è¡¨ï¼ˆçº¯æ–‡æœ¬ï¼‰
        success_urls_filename = f"success_urls_{timestamp}.txt"
        with open(success_urls_filename, 'w', encoding='utf-8') as f:
            for result in self.results:
                if result['status'] == 'completed':
                    f.write(result['url'] + "\n")
        output_files.append(success_urls_filename)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°ä»¥ä¸‹æ–‡ä»¶:")
        for filename in output_files:
            print(f"   - {filename}")

def main():
    parser = argparse.ArgumentParser(description='æ‰¹é‡URLçˆ¬å–å·¥å…·')
    parser.add_argument('input_file', help='åŒ…å«URLçš„è¾“å…¥æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªURLï¼‰')
    parser.add_argument('--api-url', default='http://localhost:5000', 
                       help='APIæœåŠ¡å™¨åœ°å€ (é»˜è®¤: http://localhost:5000)')
    parser.add_argument('--api-key', default='demo-api-key-123', 
                       help='APIå¯†é’¥ (é»˜è®¤: demo-api-key-123)')
    parser.add_argument('--threads', type=int, default=3, 
                       help='å·¥ä½œçº¿ç¨‹æ•°é‡ (é»˜è®¤: 3)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.input_file):
        print(f"é”™è¯¯: æ–‡ä»¶ '{args.input_file}' ä¸å­˜åœ¨")
        return
    
    # åˆ›å»ºçˆ¬å–å™¨å®ä¾‹
    scraper = URLScraper(api_url=args.api_url, api_key=args.api_key)
    
    # å¼€å§‹å¤„ç†
    try:
        scraper.process_urls_from_file(args.input_file, args.threads)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()