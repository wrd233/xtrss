#!/usr/bin/env python3
"""
ç«é€ŸWorker - å¤šçˆ¬è™«ç«é€Ÿå¤„ç†å¤±è´¥çš„ä»»åŠ¡
"""

import time
import logging
import signal
import sys
from datetime import datetime
from typing import Optional, Dict, List, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

from config import Config, TaskStatus
from redis_client import RedisClient
from simple_scrapers import scrape_with_scraper, SCRAPERS

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=getattr(logging, Config.LOG_LEVEL),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RaceWorker:
    """ç«é€ŸWorker - å¤„ç†requestså¤±è´¥çš„ä»»åŠ¡"""
    
    def __init__(self):
        self.redis_client = RedisClient()
        self.running = True
        self.current_task_id: Optional[str] = None
        logger.info(f"RaceWorkeråˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒçš„çˆ¬è™«: {list(SCRAPERS.keys())}")
    
    def signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†"""
        logger.info(f"æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­...")
        self.running = False
    
    def process_race_task(self, task_id: str) -> bool:
        """å¤„ç†ç«é€Ÿä»»åŠ¡ - å¤šä¸ªçˆ¬è™«åŒæ—¶å°è¯•"""
        try:
            logger.info(f"å¼€å§‹ç«é€Ÿå¤„ç†ä»»åŠ¡: {task_id}")
            
            # è·å–ä»»åŠ¡è¯¦æƒ…
            task_data = self.redis_client.get_task(task_id)
            if not task_data:
                logger.error(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
                return False
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç«é€Ÿä»»åŠ¡
            if task_data['scraper_type'] != 'race':
                logger.info(f"è·³è¿‡éç«é€Ÿä»»åŠ¡: {task_id}")
                return True
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            self.redis_client.update_task_status(
                task_id, 
                TaskStatus.PROCESSING, 
                progress=20
            )
            
            urls = task_data['urls']
            options = task_data.get('options', {})
            
            logger.info(f"ç«é€Ÿä»»åŠ¡è¯¦æƒ…: {len(urls)}ä¸ªURL")
            
            # ä½¿ç”¨çº¿ç¨‹æ± è®©æ‰€æœ‰çˆ¬è™«åŒæ—¶ç«äº‰
            first_success = None
            winner_scraper = None
            
            # å¯ç”¨çš„çˆ¬è™«ç±»å‹ï¼ˆæ’é™¤requestsï¼Œå› ä¸ºå®ƒå·²ç»è¯•è¿‡äº†ï¼‰
            race_scrapers = ['newspaper', 'readability', 'trafilatura']
            
            with ThreadPoolExecutor(max_workers=len(race_scrapers)) as executor:
                # æäº¤æ‰€æœ‰çˆ¬è™«ä»»åŠ¡
                future_to_scraper = {
                    executor.submit(self.scrape_with_scraper, urls, scraper_type): scraper_type
                    for scraper_type in race_scrapers
                }
                
                # ç­‰å¾…ç¬¬ä¸€ä¸ªæˆåŠŸçš„ç»“æœ
                for future in as_completed(future_to_scraper, timeout=45):  # 45ç§’è¶…æ—¶
                    scraper_type = future_to_scraper[future]
                    try:
                        results = future.result(timeout=5)
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸçš„çˆ¬å–
                        success_count = sum(1 for r in results if r.get('success', False))
                        
                        logger.info(f"[{scraper_type}] å®Œæˆçˆ¬å–ï¼ŒæˆåŠŸ: {success_count}/{len(results)}")
                        
                        if success_count > 0:
                            logger.info(f"ğŸ¯ [{scraper_type}] ç‡å…ˆå®Œæˆï¼Œä½¿ç”¨å…¶ç»“æœ!")
                            first_success = results
                            winner_scraper = scraper_type
                            
                            # å–æ¶ˆå…¶ä»–è¿˜åœ¨è¿è¡Œçš„ä»»åŠ¡
                            for f in future_to_scraper:
                                if f != future and not f.done():
                                    f.cancel()
                            break
                        
                    except Exception as e:
                        logger.error(f"[{scraper_type}] çˆ¬å–å¼‚å¸¸: {e}")
            
            # æ£€æŸ¥ç«é€Ÿç»“æœ
            if first_success:
                # æœ‰çˆ¬è™«æˆåŠŸï¼Œå­˜å‚¨ç»“æœ
                self.redis_client.store_results(task_id, first_success)
                self.redis_client.update_task_status(
                    task_id, 
                    TaskStatus.COMPLETED, 
                    progress=100
                )
                logger.info(f"âœ… ç«é€Ÿä»»åŠ¡å®Œæˆ: {task_id}, è·èƒœçˆ¬è™«: {winner_scraper}")
                return True
            else:
                # æ‰€æœ‰çˆ¬è™«éƒ½å¤±è´¥
                logger.warning(f"âŒ æ‰€æœ‰çˆ¬è™«éƒ½å¤±è´¥: {task_id}")
                self.redis_client.update_task_status(
                    task_id,
                    TaskStatus.FAILED,
                    error_message="æ‰€æœ‰çˆ¬è™«å°è¯•å‡å¤±è´¥"
                )
                return False
                
        except Exception as e:
            logger.error(f"ç«é€Ÿå¤„ç†ä»»åŠ¡å¤±è´¥ {task_id}: {str(e)}")
            self.redis_client.update_task_status(
                task_id,
                TaskStatus.FAILED,
                error_message=f"ç«é€Ÿå¤„ç†å¼‚å¸¸: {str(e)}"
            )
            return False
    
    def scrape_with_scraper(self, urls: List[str], scraper_type: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æŒ‡å®šçˆ¬è™«çˆ¬å–URLåˆ—è¡¨"""
        logger.info(f"[{scraper_type}] å¼€å§‹æ‰¹é‡çˆ¬å–: {len(urls)}ä¸ªURL")
        
        results = []
        successful_count = 0
        failed_count = 0
        
        for i, url in enumerate(urls):
            try:
                logger.info(f"[{scraper_type}] çˆ¬å–è¿›åº¦: {i+1}/{len(urls)} - {url}")
                
                result = scrape_with_scraper(url, scraper_type)
                results.append(result)
                
                if result.get('success', False):
                    successful_count += 1
                else:
                    failed_count += 1
                    
            except Exception as e:
                failed_count += 1
                logger.error(f"[{scraper_type}] çˆ¬å–å¼‚å¸¸: {url} - {str(e)}")
                results.append({
                    'url': url,
                    'success': False,
                    'error': str(e),
                    'scraper_type': scraper_type
                })
        
        logger.info(f"[{scraper_type}] æ‰¹é‡çˆ¬å–å®Œæˆ: æˆåŠŸ{successful_count}ä¸ª, å¤±è´¥{failed_count}ä¸ª")
        return results
    
    def get_next_race_task(self) -> Optional[str]:
        """ä»ç«é€Ÿé˜Ÿåˆ—è·å–ä»»åŠ¡"""
        # ä½¿ç”¨ä¸åŒçš„é˜Ÿåˆ—åç§°
        task_id = self.redis_client.redis_client.brpop('race_queue', timeout=Config.POLL_INTERVAL)
        if task_id:
            return task_id[1]
        return None
    
    def run(self):
        """è¿è¡ŒRaceWorkerä¸»å¾ªç¯"""
        logger.info("RaceWorkerå¼€å§‹è¿è¡Œ...")
        
        # æ³¨å†Œä¿¡å·å¤„ç†
        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)
        
        try:
            while self.running:
                try:
                    # ä»ç«é€Ÿé˜Ÿåˆ—è·å–ä»»åŠ¡
                    task_id = self.get_next_race_task()
                    
                    if task_id:
                        self.current_task_id = task_id
                        logger.info(f"ç«é€ŸWorkerè·å–åˆ°æ–°ä»»åŠ¡: {task_id}")
                        
                        # å¤„ç†ç«é€Ÿä»»åŠ¡
                        success = self.process_race_task(task_id)
                        
                        if success:
                            logger.info(f"ç«é€Ÿä»»åŠ¡å¤„ç†æˆåŠŸ: {task_id}")
                        else:
                            logger.error(f"ç«é€Ÿä»»åŠ¡å¤„ç†å¤±è´¥: {task_id}")
                        
                        self.current_task_id = None
                    else:
                        # æ²¡æœ‰ä»»åŠ¡ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´
                        logger.debug("ç«é€Ÿé˜Ÿåˆ—æ²¡æœ‰æ–°ä»»åŠ¡ï¼Œç­‰å¾…ä¸­...")
                        time.sleep(Config.POLL_INTERVAL)
                
                except KeyboardInterrupt:
                    logger.info("ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡º...")
                    break
                except Exception as e:
                    logger.error(f"RaceWorkerå¾ªç¯é”™è¯¯: {str(e)}")
                    time.sleep(Config.POLL_INTERVAL)
        
        finally:
            logger.info("RaceWorkerå·²åœæ­¢")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("æ­£åœ¨æ¸…ç†RaceWorkerèµ„æº...")
        if self.current_task_id:
            try:
                self.redis_client.update_task_status(
                    self.current_task_id,
                    TaskStatus.FAILED,
                    error_message="RaceWorkerå¼‚å¸¸åœæ­¢"
                )
            except Exception as e:
                logger.error(f"æ¸…ç†ä»»åŠ¡çŠ¶æ€æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        worker = RaceWorker()
        worker.run()
    except Exception as e:
        logger.error(f"RaceWorkerå¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)
    finally:
        if 'worker' in locals():
            worker.cleanup()

if __name__ == "__main__":
    main()